{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYtLrO-72uzi",
        "outputId": "3a0902ae-bce7-4adf-e5c6-fa0cc72a7545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.7.1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<1 (from asyncpraw)\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.1)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting asyncprawcore<3,>=2.1 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Collecting update-checker>=0.18 (from asyncpraw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2023.11.17)\n",
            "Installing collected packages: aiosqlite, aiofiles, update-checker, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-0.8.0 aiosqlite-0.17.0 asyncpraw-7.7.1 asyncprawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install asyncpraw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import asyncpraw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "async def fetch_comments(comment, comments_list, processed_ids, submission_id, indent=0):\n",
        "    await comment.refresh()\n",
        "    for reply in comment.replies:\n",
        "        if reply.id in processed_ids:\n",
        "            continue\n",
        "        processed_ids.add(reply.id)\n",
        "\n",
        "        comment_date = datetime.utcfromtimestamp(reply.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        comments_list.append({\n",
        "            'submission_id': submission_id,\n",
        "            'comment_id': reply.id,\n",
        "            'parent_id': comment.id if indent > 0 else '',\n",
        "            'body': reply.body,\n",
        "            'author': str(reply.author),\n",
        "            'created_utc': comment_date,\n",
        "            'indent': indent\n",
        "        })\n",
        "        await fetch_comments(reply, comments_list, processed_ids, submission_id, indent=indent + 1)\n",
        "\n",
        "async def main():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id='26I6lu5xjL2TW6B1apvokA',\n",
        "        client_secret='_nHSygU8rvx8J18Hp4eHH-PbN_jEvw',\n",
        "        user_agent='SummativeAPI by WorryOk9238 (Contact: juliemorrisonjp@gmail.com)',\n",
        "    )\n",
        "\n",
        "    submission_id = '11ylnft'\n",
        "    submission = await reddit.submission(id=submission_id)\n",
        "    await submission.load()\n",
        "\n",
        "    submission_date = datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    await submission.comments.replace_more(limit=None)\n",
        "    comments_list = []\n",
        "    processed_ids = set()\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "        if comment.id in processed_ids:\n",
        "            continue\n",
        "        processed_ids.add(comment.id)\n",
        "\n",
        "        comment_date = datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        comments_list.append({\n",
        "            'submission_id': submission_id,\n",
        "            'comment_id': comment.id,\n",
        "            'parent_id': '',\n",
        "            'body': comment.body,\n",
        "            'author': str(comment.author),\n",
        "            'created_utc': comment_date,\n",
        "            'indent': 0\n",
        "        })\n",
        "        await fetch_comments(comment, comments_list, processed_ids, submission_id)\n",
        "\n",
        "    df = pd.DataFrame(comments_list)\n",
        "\n",
        "    await reddit.close()\n",
        "\n",
        "    return df\n",
        "\n",
        "async def run():\n",
        "    df = await main()\n",
        "    df.to_csv('4.csv', index=False)\n",
        "    print(df)\n",
        "\n",
        "await run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRhRs3rL3rBa",
        "outputId": "60b29fca-8ec5-4b1d-cfb6-2ed3bf6dca9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   submission_id comment_id parent_id  \\\n",
            "0        11ylnft    jd8904k             \n",
            "1        11ylnft    jd8c55n             \n",
            "2        11ylnft    jdb38a3   jd8c55n   \n",
            "3        11ylnft    juymrnf   jd8c55n   \n",
            "4        11ylnft    juytvhn   juymrnf   \n",
            "5        11ylnft    jdb2wuz             \n",
            "6        11ylnft    jd8s0fg             \n",
            "7        11ylnft    jdbd52y             \n",
            "8        11ylnft    jji7vyk             \n",
            "9        11ylnft    jd8wtm1             \n",
            "10       11ylnft    jdb5ciq             \n",
            "11       11ylnft    jd94zs2             \n",
            "12       11ylnft    jdb5upa             \n",
            "13       11ylnft    jlz16nu   jdb5upa   \n",
            "14       11ylnft    jdbdfnr             \n",
            "15       11ylnft    jda8i5r             \n",
            "16       11ylnft    jdb6bup             \n",
            "17       11ylnft    jdbbzm1             \n",
            "18       11ylnft    jdc57za   jdbbzm1   \n",
            "19       11ylnft    jd8w7nk             \n",
            "20       11ylnft    jd9fpre             \n",
            "21       11ylnft    jdbcfvt             \n",
            "22       11ylnft    jdbdiv3             \n",
            "23       11ylnft    jd9l61x             \n",
            "24       11ylnft    jdbekbu             \n",
            "25       11ylnft    jdbi6eu             \n",
            "26       11ylnft    jd8cb7v             \n",
            "27       11ylnft    jd8oa11             \n",
            "28       11ylnft    jd8osnv   jd8oa11   \n",
            "29       11ylnft    jd8q4dp   jd8osnv   \n",
            "30       11ylnft    jd8rirl   jd8q4dp   \n",
            "31       11ylnft    jdab2pm   jd8q4dp   \n",
            "32       11ylnft    jdb9bno   jd8q4dp   \n",
            "33       11ylnft    jdb6kez             \n",
            "34       11ylnft    jda7ps1             \n",
            "35       11ylnft    jdbdxr3             \n",
            "36       11ylnft    jdatod2             \n",
            "37       11ylnft    jdbedfv             \n",
            "38       11ylnft    jdmsqba   jdbedfv   \n",
            "39       11ylnft    jdcukbr             \n",
            "40       11ylnft    jdddtrn             \n",
            "41       11ylnft    jdb2gyd             \n",
            "42       11ylnft    jdb51tm             \n",
            "43       11ylnft    jdb7xfr             \n",
            "44       11ylnft    jdb9xym             \n",
            "45       11ylnft    jdbaq7q             \n",
            "46       11ylnft    jdcgngh             \n",
            "47       11ylnft    jde6xg0             \n",
            "48       11ylnft    jdkp64f             \n",
            "49       11ylnft    jdlkji0             \n",
            "50       11ylnft    jdlm4np   jdlkji0   \n",
            "51       11ylnft    jdqu39s             \n",
            "52       11ylnft    k34gma5             \n",
            "53       11ylnft    kab7dgh             \n",
            "\n",
            "                                                 body                author  \\\n",
            "0                                           [deleted]                  None   \n",
            "1   Employing ghost writers will soon become a thi...  AuspiciouslyAutistic   \n",
            "2   This is a great point ! It will be so nice to ...      Dazzling-Diva100   \n",
            "3   Considering there’s a possibility that “ghostw...        TheGeekyWriter   \n",
            "4   It's one job of many across various industries...  AuspiciouslyAutistic   \n",
            "5   I agree it’s an amazing tool and people can de...      Dazzling-Diva100   \n",
            "6   It's never going to happen fully with books. N...          Celestin_Sky   \n",
            "7   I see the initial value of AI in its access to...      Dazzling-Diva100   \n",
            "8   Don't speak too soon. This is just the beginni...  DreamApprehensive639   \n",
            "9                     No it will replace editors lol.           TheIndyCity   \n",
            "10                                      Very likely !      Dazzling-Diva100   \n",
            "11  Remember, almost no one is actually able to ap...          XtremelyMeta   \n",
            "12  I agree. Although I think AI authored books co...      Dazzling-Diva100   \n",
            "13  a big thing that makes book interesting is tha...            Osanshoouo   \n",
            "14                                         Well said.      Dazzling-Diva100   \n",
            "15  I think writers will use it for shortcuts and ...          FlyLikeMouse   \n",
            "16  I agree. The human experience is something we ...      Dazzling-Diva100   \n",
            "17  Maybe overtime AI systems will learn how to be...      Dazzling-Diva100   \n",
            "18  Yeah, for example, it cant handle any real wit...          FlyLikeMouse   \n",
            "19  As a person who writes hard science fiction ab...           Netcentrica   \n",
            "20                        I hope this isn’t a thing..    Educational_Ice151   \n",
            "21  I think it will be whatever we make it. How we...      Dazzling-Diva100   \n",
            "22               I hear you… it’s a little out there.      Dazzling-Diva100   \n",
            "23  Right now. You can use chatgpt API as your gho...              alexiuss   \n",
            "24              So cool… I am going to check it out !      Dazzling-Diva100   \n",
            "25  This is such an amazing breakthrough. I think ...      Dazzling-Diva100   \n",
            "26  At first it will be used as a tool by fiction ...              Spirckle   \n",
            "27  So, what’s your prognosis? When are writers go...         earthyterry49   \n",
            "28  large scale?  By the big publishers?  2-3 year...              Spirckle   \n",
            "29  Oh, so you think that the progress of AI will ...         earthyterry49   \n",
            "30  Yes, 2-3 years estimate takes into account tha...              Spirckle   \n",
            "31  I think people praise the writing too highly. ...          FlyLikeMouse   \n",
            "32  I am sure AI will be used to produce amazing s...      Dazzling-Diva100   \n",
            "33  It will be interesting to see how distinguisha...      Dazzling-Diva100   \n",
            "34  I recently watched a video Phil Edwards made o...              IFKarona   \n",
            "35  I appreciate that AI can provide help in so ma...      Dazzling-Diva100   \n",
            "36  Probably never tbh. An AI can certainly write ...    Office_Depot_wagie   \n",
            "37  I agree but I have heard that ChatGPT can prov...      Dazzling-Diva100   \n",
            "38  Incredible tool, not to be used as a crutch th...    Office_Depot_wagie   \n",
            "39  Don't know but as the scenario is, it might be...            nyramiller   \n",
            "40      So - what should writers do? Your suggestion?         earthyterry49   \n",
            "41  AI is capable of creating stories and probably...      Dazzling-Diva100   \n",
            "42  I think fiction writers can use AI as a tool t...      Dazzling-Diva100   \n",
            "43                     I agree, certainly in fiction.      Dazzling-Diva100   \n",
            "44  I am sure AI will generate amazing books but y...      Dazzling-Diva100   \n",
            "45  There are many ways that AI can be helpful and...      Dazzling-Diva100   \n",
            "46                                               2022          _-_agenda_-_   \n",
            "47  It won't replace authors at all. Even if AI be...              fox22usa   \n",
            "48  Replace outright: 1-2 years (possibly sooner a...    anonymouswriter777   \n",
            "49  But you see, a LLM requires a large body of te...         earthyterry49   \n",
            "50  That's a great point. High quality datasets wi...    anonymouswriter777   \n",
            "51  I think people naturally pick the best and eas...      Dazzling-Diva100   \n",
            "52  AI can be of great help...but I don't think AI...            MemesGuyAI   \n",
            "53  I appreciate the thoughtful question. As an au...               illyism   \n",
            "\n",
            "            created_utc  indent  \n",
            "0   2023-03-22 15:26:47       0  \n",
            "1   2023-03-22 15:46:43       0  \n",
            "2   2023-03-23 02:49:26       1  \n",
            "3   2023-08-05 23:14:49       1  \n",
            "4   2023-08-06 00:09:08       2  \n",
            "5   2023-03-23 02:46:51       0  \n",
            "6   2023-03-22 17:25:46       0  \n",
            "7   2023-03-23 04:15:48       0  \n",
            "8   2023-05-09 18:59:45       0  \n",
            "9   2023-03-22 17:55:43       0  \n",
            "10  2023-03-23 03:06:40       0  \n",
            "11  2023-03-22 18:47:00       0  \n",
            "12  2023-03-23 03:10:45       0  \n",
            "13  2023-05-28 18:16:56       1  \n",
            "14  2023-03-23 04:18:39       0  \n",
            "15  2023-03-22 23:06:17       0  \n",
            "16  2023-03-23 03:14:44       0  \n",
            "17  2023-03-23 04:04:44       0  \n",
            "18  2023-03-23 10:29:30       1  \n",
            "19  2023-03-22 17:51:59       0  \n",
            "20  2023-03-22 19:54:30       0  \n",
            "21  2023-03-23 04:09:05       0  \n",
            "22  2023-03-23 04:19:30       0  \n",
            "23  2023-03-22 20:29:04       0  \n",
            "24  2023-03-23 04:29:43       0  \n",
            "25  2023-03-23 05:08:01       0  \n",
            "26  2023-03-22 15:47:47       0  \n",
            "27  2023-03-22 17:02:23       0  \n",
            "28  2023-03-22 17:05:42       1  \n",
            "29  2023-03-22 17:13:59       2  \n",
            "30  2023-03-22 17:22:44       3  \n",
            "31  2023-03-22 23:23:42       3  \n",
            "32  2023-03-23 03:40:27       3  \n",
            "33  2023-03-23 03:16:43       0  \n",
            "34  2023-03-22 23:00:44       0  \n",
            "35  2023-03-23 04:23:34       0  \n",
            "36  2023-03-23 01:37:07       0  \n",
            "37  2023-03-23 04:27:48       0  \n",
            "38  2023-03-25 15:49:39       1  \n",
            "39  2023-03-23 14:15:16       0  \n",
            "40  2023-03-23 16:21:00       0  \n",
            "41  2023-03-23 02:43:18       0  \n",
            "42  2023-03-23 03:04:13       0  \n",
            "43  2023-03-23 03:28:14       0  \n",
            "44  2023-03-23 03:46:00       0  \n",
            "45  2023-03-23 03:53:00       0  \n",
            "46  2023-03-23 12:28:43       0  \n",
            "47  2023-03-23 19:23:45       0  \n",
            "48  2023-03-25 02:21:35       0  \n",
            "49  2023-03-25 08:13:09       0  \n",
            "50  2023-03-25 08:37:07       1  \n",
            "51  2023-03-26 13:56:19       0  \n",
            "52  2023-10-02 10:45:54       0  \n",
            "53  2023-11-22 15:37:24       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = ['1.csv', '2.csv', '3.csv', '4.csv']\n",
        "\n",
        "dfs = [pd.read_csv(file) for file in csv_files]\n",
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "combined_df.to_csv('reddit_netnography_comments.csv', index=False)\n"
      ],
      "metadata": {
        "id": "o4l1sBIHBbrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}